{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PNL_Introduccion.ipynb","private_outputs":true,"provenance":[{"file_id":"1b7RVWoWHIncaipkXJ7FXiTAR9ggSRAgt","timestamp":1604063989215}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"JoKzNdNcuVgj"},"source":["# Introducción\n","\n","Un modelo matemático de optimización no lineal, al igual que los modelos lineales, estan compuestos de variables, objetivos y restricciones. Sin embargo, el modelo no lineal considera almenos una función no lineal de las variables, bien sea en la función objetivo o en algunas (o todas) las restricciones. \n","\n","Es de utilidad primero comprender porque los programas no lineales son usualmente más dificiles de resolver que su contraparte lienal. La siguiente \n","<a href=\"https://drive.google.com/file/d/16QgnWiob56N4JJGFLAnb8Gg5VRqvHgNY/view?usp=sharing\">animación </a> resume las principales razones, presentando un resumen del **Capitulo 16** del libro <a href=\"https://drive.google.com/file/d/16QgnWiob56N4JJGFLAnb8Gg5VRqvHgNY/view?usp=sharing\">Practical optimization: a gentle introduction </a>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"u3vLRLK4y9IO"},"source":["# Taxonomias de los programas no lineales"]},{"cell_type":"markdown","metadata":{"id":"imeLTm7ozThO"},"source":["## Según el tipo de problema\n","\n","Los programas no lineales puede clasificarse en primer lugar dependiendo del tipo de problemas\n","\n","<img src=\"https://docs.google.com/uc?export=download&id=1byy5Enil7G4vsLiBLlt6a3gOu1DeWULG\" alt=\"Drawing\" style=\"width: 100px;\"/>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"uc95L0hl0HY_"},"source":["## Según el método de solución\n","\n","Los métodos de solución, dependerá del tipo de problema considerado, algunos de los más comunes son: \n","\n","<img src=\"https://docs.google.com/uc?export=download&id=11ayZvr5jmypWAAcTygxoWjDbMoJEajT0\" alt=\"Drawing\" style=\"width: 100px;\"/>\n","\n","<img src=\"https://docs.google.com/uc?export=download&id=1CXwoCrJ7qZDHV1mcz5VymtZb1oQpyAYd\" alt=\"Drawing\" style=\"width: 100px;\"/>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"l2APj8mN-F8C"},"source":["# Conceptos básicos y programas no lineales sin restricciones\n","\n","Iniciaremos el estudio abordando los programas no lineales sin restricciones. Los elementos básicos que aquí se describen serán la base para programas más complejos en los que se consideran  restricciones.\n","\n","Para ello usted deberá leer las **secciones 16.1 a 16.6** del libro  <a href=\"https://drive.google.com/file/d/1kQl-Tw50usXGBuepoGw7Cc9G6AXId34W//view?usp=sharing\">Optimization in Operations Research </a>\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xG3TZQj2yTsy"},"source":["# Actividad de clase 1\n","\n","Con base en las lecturas realizadas usted deberá formular tres preguntas respecto a los conceptos básicos. \n","\n","> **Tip metodológico**: La taxonomia de bloom para el ámbito cognitivo permite evidenciar el nivel de apropiación del conocmiento. Siendo frecuentemente utilizada como referente para la formulación de objetivos o el diseño de elementos de aprendizaje <img src=\"https://docs.google.com/uc?export=download&id=1l-8iqeFI_CkIlpAUWYPhncAEiV64rIfz\" alt=\"Drawing\" style=\"width: 200px;\"/>\n","\n","Las preguntas que usted formulará deben estar ubicadas en las tres primeras categorias de profundidad del conocimiento de acuerdo con la taxonomia de bloom (Recordar, comprender, aplicar). Agregue sus preguntas en el siguiente <a href=\"https://drive.google.com/file/d/10B6ASUVOkUZx83ufhEiXCJif78huirgk401Pu81nSG4//view?usp=sharing\">archivo.</a>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"yPh20fOsAbMN"},"source":["# Algoritmos de solución"]},{"cell_type":"markdown","metadata":{"id":"B34KGpWrAolE"},"source":["## Búsqueda de la sección dorada (Golden section search)\n","El algoritmo de la sección dorada es  algoritmo es aplicable para:\n","* Problemas no restringidos\n","* funciones objetivo univariadas\n","* funciones objetivo unimodaless \n","\n","\n","El siguiente pseudocódigo tomado del libro de Rardin describe el algoritmo de búsqueda de sección dorada\n","<img src=\"https://docs.google.com/uc?export=download&id=194Mjepy4HhCfu4LXrpVrw6vp__h_46sc\" alt=\"Drawing\" style=\"width: 200px;\"/>. \n","\n","Usaremos la implementación del algoritmo disponible en la libreria `scipy`\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"kJlg1AKUCcwh"},"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# define una función \n","def function(x):\n","  return x**4 - 2*x**3 - x**2 + 5*x\n","\n","x = np.linspace(-4.0, 4.0, 200)\n","y = function(x)\n","plt.plot(x, y)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pU7f3_mYbCVe"},"source":["Definamos el intervalo (bracket) en el que se encuentra el óptimo de la función y optimiza\n"]},{"cell_type":"code","metadata":{"id":"Zv7qR7oYbidO"},"source":["lo = -4\n","mid = 0\n","up = 4\n","absolutePrecision = 1e-10\n","\n","from scipy import optimize\n","minimum = optimize.golden(function, brack=(lo, mid, up))\n","minimum"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MNpvYMXQs9ha"},"source":["### Actividad de clase 2. \n","\n","Experimente con el algoritmo usando distintas funciones que usted mismo defina. Analice los resultados"]},{"cell_type":"code","metadata":{"id":"LnLiYiJTEd0K"},"source":["# Inserte su código aquí\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1hgYmoI3_qYZ"},"source":["## Búsqueda de gradiente multidimensional (bidimensional)"]},{"cell_type":"markdown","metadata":{"id":"31p7FIFKDvy7"},"source":["El algoritmo de búsqueda gradiente usa información de las primeras derivadas (gradiente) para orientar la dirección de búsqueda de mejores soluciones. \n","\n","El siguiente pseudocódigo tomado del libro de Rardin describe el algoritmo de búsqueda gradiente\n","<img src=\"https://docs.google.com/uc?export=download&id=1ERfKlE_5XZY_Xq0GTmfag0VqII2F6Pt1\" alt=\"Drawing\" style=\"width: 200px;\"/>. \n","\n","Usaremos la implementación del algoritmo descrita en este <a href=\"https://xavierbourretsicotte.github.io/Intro_optimization.html\">enlace</a>.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"IzvqaslOEhRE"},"source":["Definimos primero los métodos que nos permitirán graficar las funciones y el proceso de optimización"]},{"cell_type":"code","metadata":{"id":"jJjQlHk3IH3-"},"source":["import numpy as np\n","import matplotlib.pyplot as plt \n","\n","# Método para graficar la función\n","def graph_function(f, x_domain, y_domain):\n","  x = np.linspace(x_domain[0],x_domain[1],250)\n","  y = np.linspace(y_domain[0],y_domain[1],250)\n","  X, Y = np.meshgrid(x, y)\n","  Z = f(X, Y)\n","\n","  %matplotlib inline\n","  fig = plt.figure(figsize = (16,8))\n","\n","  #Surface plot\n","  ax = fig.add_subplot(1, 2, 1, projection='3d')\n","  ax.plot_surface(X,Y,Z,rstride = 5, cstride = 5, cmap = 'jet', alpha = .4, edgecolor = 'none')\n","  plt.show()\n","\n","# Método para graficar la trayectoria de optimización\n","def graph_opt(fun, x_domain, y_domain, iter_x, iter_y):\n","  x = np.linspace(x_domain[0],x_domain[1],250)\n","  y = np.linspace(y_domain[0],y_domain[1],250)\n","  X, Y = np.meshgrid(x, y)\n","  Z = fun(X, Y)\n","\n","  #Angles needed for quiver plot\n","  anglesx = iter_x[1:] - iter_x[:-1]\n","  anglesy = iter_y[1:] - iter_y[:-1]\n","\n","  %matplotlib inline\n","  fig = plt.figure(figsize = (16,8))\n","\n","  #Surface plot\n","  ax = fig.add_subplot(1, 2, 1, projection='3d')\n","  ax.plot_surface(X,Y,Z,rstride = 5, cstride = 5, cmap = 'jet', alpha = .4, edgecolor = 'none' )\n","  ax.plot(iter_x,iter_y, fun(iter_x,iter_y),color = 'r', marker = '*', alpha = .4)\n","\n","  #Rotate the initialization to help viewing the graph\n","  ax.view_init(45, 280)\n","  ax.set_xlabel('x')\n","  ax.set_ylabel('y')\n","\n","  #Contour plot\n","  ax = fig.add_subplot(1, 2, 2)\n","  ax.contour(X,Y,Z, 60, cmap = 'jet')\n","  #Plotting the iterations and intermediate values\n","  ax.scatter(iter_x,iter_y,color = 'r', marker = '*')\n","  ax.quiver(iter_x[:-1], iter_y[:-1], anglesx, anglesy, scale_units = 'xy', angles = 'xy', scale = 1, color = 'r', alpha = .3)\n"," \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cd-4xfMoFUVS"},"source":["Definimos el método que ejecuta el algoritmo de gradiente.\n","\n","**Note** que este método considera una longitud de paso (parámetro *gamma*) fijo, lo cual debería modificarse en cada iteración"]},{"cell_type":"code","metadata":{"id":"R2ibOLnfEag_"},"source":["import numpy as np\n","def Gradient_Descent(Grad,x,y, gamma = 0.00125, epsilon=0.0001, nMax = 10000 ):\n","    #Initialization\n","    i = 0\n","    iter_x, iter_y, iter_count = np.empty(0),np.empty(0), np.empty(0)\n","    error = 10\n","    X = np.array([x,y])\n","    \n","    #Looping as long as error is greater than epsilon\n","    while np.linalg.norm(error) > epsilon and i < nMax:\n","        i +=1\n","        iter_x = np.append(iter_x,x)\n","        iter_y = np.append(iter_y,y)\n","        iter_count = np.append(iter_count ,i)         \n","        X_prev = X\n","        X = X - gamma * Grad(x,y)\n","        error = X - X_prev\n","        x,y = X[0], X[1]        \n","    \n","    return X, iter_x,iter_y, iter_count\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SXpGM9W4GaAz"},"source":["### Ejemplo 1\n","\n","La <a href=\"https://es.wikipedia.org/wiki/Funci%C3%B3n_de_Rosenbrock\">función de Rosenbrock </a> es una función no convexa utilizada como problema de prueba del rendimiento para algoritmos de optimización que se introdujo por Howard H. Rosenbrock en 1960.1​ Es también conocida como Rosenbrock la función del valle o la función del plátano.\n","\n","El mínimo global está dentro de un valle plano, largo, estrecho y de forma parabólica. Encontrar el valle es trivial. Sin embargo, converger al mínimo global es difícil.\n","\n","La función está definida por:\n","\n","$\\displaystyle f(x,y)=(a-x)^{2}+b(y-x^{2})^{2}$\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"MJIFKFLbHpuU"},"source":["def Rosenbrock(x,y):\n","    return (1 + x)**2 + 100*(y - x**2)**2\n","\n","def Grad_Rosenbrock(x,y):\n","    g1 = -400*x*y + 400*x**3 + 2*x -2\n","    g2 = 200*y -200*x**2\n","    return np.array([g1,g2])\n","\n","def Hessian_Rosenbrock(x,y):\n","    h11 = -400*y + 1200*x**2 + 2\n","    h12 = -400 * x\n","    h21 = -400 * x\n","    h22 = 200\n","    return np.array([[h11,h12],[h21,h22]])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TyU7h1lqH9PN"},"source":["Graficamos la función"]},{"cell_type":"code","metadata":{"id":"hcFslvCFIAdF"},"source":["graph_function(Rosenbrock, (-3,3), (-10,8))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SztXh88HKj7L"},"source":["Desarrollar el proceso de optimización y graficar la trayectoria"]},{"cell_type":"code","metadata":{"id":"zrCnoZsDH3yx"},"source":["root,iter_x,iter_y, iter_count = Gradient_Descent(Grad_Rosenbrock,-2,2)\n","graph_opt(Rosenbrock, (-3,3), (-10,8), iter_x, iter_y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jvEaKRuPMeg7"},"source":["### Ejemplo 2\n","\n","La función de Himmelblau es una función multi-modal, definida sobre \\mathbb{R}^2 y usada para comprobar el rendimiento de los algoritmos de optimización.\n","\n","La función se define de la siguiente manera:\n","\n","e function is defined by:\n","\n","$\\displaystyle f(x,y)=(x^{2}+y-11)^{2}+(x+y^{2}-7)^{2}$\n","\n"]},{"cell_type":"code","metadata":{"id":"3aN_0p_QM7-W"},"source":["def Himmer(x,y):\n","    return (x**2 + y - 11)**2 + ( x + y**2 - 7 )**2\n","\n","def Grad_Himmer(x,y):\n","    return np.array([2 * (-7 + x + y**2 + 2 * x * (-11 + x**2 + y)), 2 * (-11 + x**2 + y + 2 * y * (-7 + x + y**2))])\n","\n","def Hessian_Himmer(x,y):\n","    h11 = 4 * (x**2 + y - 11) + 8 * x**2 + 2\n","    h12 = 4 * x + 4 * y\n","    h21 = 4 * x + 4 * y \n","    h22 = 4 * (x + y**2 - 7) + 8 * y**2 + 2\n","    \n","    return np.array([[h11,h12],[h21,h22]]) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ChNdR5NsNCyT"},"source":["Gráficamos la función"]},{"cell_type":"code","metadata":{"id":"KSqnTta2NFb_"},"source":["graph_function(Himmer, (-5,5), (-5,5))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TIw2-abPNqoY"},"source":["Desarrollar el proceso de optimización y graficar la trayectoria"]},{"cell_type":"code","metadata":{"id":"UJSWFROoNxN2"},"source":["root,iter_x,iter_y, iter_count = Gradient_Descent(Grad_Himmer,-1,1)\n","graph_opt(Himmer, (-5,5), (-5,5), iter_x, iter_y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IgJkpeKIOLSe"},"source":["### Actividad de clase 3\n","Experimente el algoritmo con sus propias funciones"]},{"cell_type":"code","metadata":{"id":"lfPlBvy5OSmU"},"source":["# Ingrese aquí sus funciones\n","\n","def function2(x,y):\n","    return (x + 3)**2 + y**2 +6"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7lUhUwOyO1He"},"source":["## Algoritmo Newton Raphson"]},{"cell_type":"markdown","metadata":{"id":"efIIy7wKEwxp"},"source":["El algoritmo de Newton Raphson complementa la información otorgada por el gradiente, mediante la segunda derivada (o matriz Hessiana) con el fin de orientar la búsqueda\n","\n","El siguiente pseudocódigo tomado del libro de Rardin describe el algoritmo Newton Raphson\n","<img src=\"https://docs.google.com/uc?export=download&id=1z73fC7Tn4Id8URyRYZD7_KvB92l0GbNk\" alt=\"Drawing\" style=\"width: 200px;\"/>. \n","\n","Usaremos la implementación del algoritmo descrita en este <a href=\"https://xavierbourretsicotte.github.io/Intro_optimization.html\">enlace</a>."]},{"cell_type":"code","metadata":{"id":"45ywGxfuuEp0"},"source":["import numpy as np\n","\n","def Newton_Raphson_Optimize(Grad, Hess, x,y, epsilon=0.000001, nMax = 200):\n","    #Initialization\n","    i = 0\n","    iter_x, iter_y, iter_count = np.empty(0),np.empty(0), np.empty(0)\n","    error = 10\n","    X = np.array([x,y])\n","    \n","    #Looping as long as error is greater than epsilon\n","    while np.linalg.norm(error) > epsilon and i < nMax:\n","        i +=1\n","        iter_x = np.append(iter_x,x)\n","        iter_y = np.append(iter_y,y)\n","        iter_count = np.append(iter_count ,i)  \n","        \n","        X_prev = X\n","        X = X - np.linalg.inv(Hess(x,y)) @ Grad(x,y)\n","        error = X - X_prev\n","        x,y = X[0], X[1]\n","          \n","    return X, iter_x,iter_y, iter_count\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z7edpkSfPDSb"},"source":["### Ejemplo 1 "]},{"cell_type":"code","metadata":{"id":"wFlJgEa_PL-X"},"source":["def Rosenbrock(x,y):\n","    return (1 + x)**2 + 100*(y - x**2)**2\n","\n","def Grad_Rosenbrock(x,y):\n","    g1 = -400*x*y + 400*x**3 + 2*x -2\n","    g2 = 200*y -200*x**2\n","    return np.array([g1,g2])\n","\n","def Hessian_Rosenbrock(x,y):\n","    h11 = -400*y + 1200*x**2 + 2\n","    h12 = -400 * x\n","    h21 = -400 * x\n","    h22 = 200\n","    return np.array([[h11,h12],[h21,h22]])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vwefzon8PNau"},"source":["Desarrollar la optimización  y graficar la trayectoria"]},{"cell_type":"code","metadata":{"id":"YfYyMVCqPULX"},"source":["root,iter_x,iter_y, iter_count = Newton_Raphson_Optimize(Grad_Rosenbrock,Hessian_Rosenbrock,-2,2)\n","graph_opt(Rosenbrock, (-3,3), (-10,8), iter_x, iter_y)\n","print(root)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ByutciZ6Rvac"},"source":["### Ejemplo 2\n"]},{"cell_type":"code","metadata":{"id":"Ji4NKz0_R31O"},"source":["def Himmer(x,y):\n","    return (x**2 + y - 11)**2 + ( x + y**2 - 7 )**2\n","\n","def Grad_Himmer(x,y):\n","    return np.array([2 * (-7 + x + y**2 + 2 * x * (-11 + x**2 + y)), 2 * (-11 + x**2 + y + 2 * y * (-7 + x + y**2))])\n","\n","def Hessian_Himmer(x,y):\n","    h11 = 4 * (x**2 + y - 11) + 8 * x**2 + 2\n","    h12 = 4 * x + 4 * y\n","    h21 = 4 * x + 4 * y \n","    h22 = 4 * (x + y**2 - 7) + 8 * y**2 + 2\n","    \n","    return np.array([[h11,h12],[h21,h22]]) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EN3jt0ddR6hj"},"source":["Desarrollar la optimización y graficar la trayectoria"]},{"cell_type":"code","metadata":{"id":"Dh8yPDnpR9l8"},"source":["root,iter_x,iter_y, iter_count = Newton_Raphson_Optimize(Grad_Himmer,Hessian_Himmer,-1,1)\n","graph_opt(Himmer, (-5,5), (-5,5), iter_x, iter_y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uty8xjZ-IVs7"},"source":["# Ejercicios"]},{"cell_type":"markdown","metadata":{"id":"TDZrewqOIXbP"},"source":["## Ejercicio 1\n","\n","El servicio de emergencias 123 desea estimar el la función de distribución de probabilidad exponencial de los tiempos entre arribos de llamadas. Para ello se tiene la siguiente muestra de tiempos entre arribos (en segundos). \n","\n","`[80, 10, 14, 26, 40, 22]`\n","\n","Tenga presente además que para la distribución exponencial $f(x) = \\lambda e ^ {\\lambda x} $.\n","\n","* Escriba la función de máxima verosimilitud que permite obtener el valor de $\\lambda$ que maximiza la probabilidad de la muestra de tiempos entre arribos suministrada.\n","* Grafique la función y obtenga el valor óptimo de $\\lambda$ por inspección visual\n","* Determine si alguno de los algoritmos considerados puede emplearse para obtener el valor óptimo de $\\lambda$."]},{"cell_type":"code","metadata":{"id":"jZiUCGxLLYQr"},"source":["# Inserte su código aquí\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jPz_3Wr393-M"},"source":["Usemos el algoritmo de la razón dorada para obtener el valor óptimo"]},{"cell_type":"code","metadata":{"id":"hjyAzKqA9G_T"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XD5dhfBXcbfs"},"source":["## Ejercicio 2\n","\n","Con el fin de mejorar el curso de optimización, el profesor esta interesado en estudiar la aprobación que sus videos  tienen por parte de sus estudiantes. De manera particular, esta interesado en conocer el impacto que tiene la duración de los videos en el número de \"me gusta\" que reciben. A continuación se presenta una muestra de datos de los videos publicados en ediciones pasadas del curso: \n","\n","|  |  | |  |  |  |  |  |  |  |  |  |  |  |  |  |\n","| -------- | --- | --- | --- | --- | --- | --- | --- | --- | ---- | --- | ---- | --- | ---- | ---- | ---- |\n","| Duración | 292 | 435 | 472 | 245 | 822 | 718 | 694 | 988 | 1325 | 819 | 1305 | 193 | 1020 | 1169 | 1048 |\n","| MeGusta  | 12  | 19  | 20  | 10  | 33  | 29  | 31  | 41  | 56   | 35  | 55   | 11  | 44   | 47   | 43   |\n","\n"]},{"cell_type":"markdown","metadata":{"id":"T5cC54amBnb9"},"source":["Los datos parecen tener una asociación lineal. Use el método de busqueda gradiente para estimar los coeficientes de la recta $y = \\beta_o + \\beta_1*x$ que permiten predecir el número de *me gusta* en función de la duración del video"]},{"cell_type":"markdown","metadata":{"id":"Rui5IAqTmS5O"},"source":["### Forma 1: Implementemos el algorimo desde cero\n","\n","Consideremos la definición del algoritmo y adaptamos la implementación descrita en este <a href=\"https://towardsdatascience.com/linear-regression-using-gradient-descent-97a6c8700931\">enlace</a>."]},{"cell_type":"code","metadata":{"id":"iirPrHrxlKvl"},"source":["# escriba su respuesta\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I7hoA9YzogoE"},"source":["### Forma 2: Modifiquemos el algoritmo que anteriormente desarrollamos"]},{"cell_type":"code","metadata":{"id":"l4mkCDXKh0Yx"},"source":["# Escriba su respuesta"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yQB_hY9gOeL4"},"source":["# Referencias\n","\n","Los algoritmos presentados en este notebook son una adaptación de los descritos en esta página: https://xavierbourretsicotte.github.io/Intro_optimization.html"]}]}